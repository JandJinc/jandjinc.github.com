{"ast":null,"code":"export default {\n  name: 'HoeAIPage.vue',\n  data() {\n    return {};\n  }\n};","map":{"version":3,"mappings":"AAwFA,eAAe;EACXA,IAAI,EAAE,eAAe;EACrBC,IAAI,GAAE;IACF,OAAO,CAEP;EACJ;AACJ","names":["name","data"],"sourceRoot":"","sources":["C:\\Users\\joost\\Desktop\\code\\informatica\\kunstmatigeintelligentie\\src\\components\\HoeAIPage.vue"],"sourcesContent":["<template>\r\n<div v-katex:auto style=\"white-space: pre-line;\">\r\nAI is lastig om te begrijpen, er zijn hele universiteiten die je leren hoe sommige dingen werken, en dan heb je niet eens alles gehad.\r\nHet makkelijkste is om je voor te stellen is een doos waar je aan de bovenkant een input, zoals een foto van een kat, in doet en het woord kat komt eruit. Je hebt geen idee hoe die doos die foto en het woord kat heeft gekoppeld, maar het werkt. \r\nOnderzoekers hebben ooit een AI laten kijken naar veel data van vallende objecten op aarde. Het doel was de AI te laten voorspellen wat er ging gebeuren in een situatie die de AI nog nooit had gezien. De AI voorspelde perfect hoe dat object ging vallen, het antwoord was dus goed, maar toen de onderzoekers gingen kijken naar hoe de AI op dat antwoord is gekomen zagen ze dat de AI allemaal niet bestaande (of onbekende) natuurkundige constanten, eenheden en formules had gebruikt. \r\nDit voorbeeld zet je ook aan het denken over of wij wel op het “goede” natuurkundige pad zitten, omdat er kennelijk ook andere constanten zijn die wij niet kennen maar wel werken\r\n\r\nMaar nu wordt het een beetje wiskundig :’)\r\n\r\nEen AI lijkt tot nu toe een doos waar je iets in doen en iets magisch komt eruit. Die magie ga ik nu uitleggen. \r\nEen neural network (AI) werkt met neurons, dat zijn een soort bolletjes waar iets ingaat en iets uitgaat. Die neuron krijgen dus inputs en die zet hij om in een output. \r\nAls meerdere neurons samen zet kunnen ze patronen in de inputs herkennen.\r\nAls je HEEL VEEL neurons samen zet kunnen ze patronen in patronen herkennen en daar kan je wat mee. \r\nMaar hoe “weten” die neurons wat voor output ze moeten sturen. Dat weten ze door de inputs en outputs te veranderen. \r\n\r\nStel je voor je wilt een bepaalde output van een neuron bij een bepaalde input \r\nEnbij  een andere combinatie van inputs een andere output:\r\nDus:\r\ninput 1 = $1, 1, 0$\r\nNodige output = 1\r\n\r\nInput 2= $0, 1, 1$\r\nNodige output 2 = 0\r\n\r\nElke neuron geeft elke input een soort belangrijkheidsgetal, die worden “weights” genoemd.\r\nIn het voorbeeld:\r\nKennelijk heeft de eerste input effect op de output (input1[0] == 1 && output == 1) (input2[0] == 0 && output == 0)\r\nDus we geven de eerste weight een “belangrijkheid” van 1\r\nDe tweede blijkt tot nu toe geen effect te hebben op de output dus we geven hem een weight van 0\r\nDe laatste lijkt een soort omgekeerd effect te hebben op de output dus we geven hem de weight van -1\r\nDus de weights zijn nu: $1, 0, -1$\r\n\r\nIk gebruikt hier [ ] om een lijst aan te geven\r\nAls ik lijst_naam[index] gebruikt verwijs ik naar het item op die index, item 1 is index 0\r\n\r\nAls je nu in de neuron de $[1, 1, 0]$ input\r\nDoet de neuron de berekening: \r\n$Input[0] *weights[0] + Input[1] *weights[1] + Input[2] *weights[2] $=\r\n$ 1 * 1 + 1 * 0 + 0 * -1 = 1 $\r\nDus de output is 1\r\nAls we nu de input $[0, 1, 1]$ nemen: \r\n$Input[0] *weights[0] + Input[1] *weights[1] + Input[2] *weights[2]$ =\r\n$ 0 * 1 + 1 * 0 + 1 * -1 = -1 $\r\n\r\nBij die laatste zien we dat hij -1 geeft terwijl 0 verwachtte. Hier is een oplossing voor.\r\nDe output (en dus ook inputs voor volgende neurons) vallen alleen tussen 0 en 1\r\nDus we moeten alle mogelijke outputs ($-\\infty$ tot $\\infty$) tussen 0 en 1 proppen.\r\nHiervoor gebruiken we een “sigmoid function” \r\n$f(x) = \\frac{x}{\\sqrt{1+x^{2}}}$\r\n\r\n\r\nDeze functie geeft voor alle input waardes een output tussen $-\\infty$ en $\\infty$\r\nDit is handig omdat je dan de uitkomst van die som van al die $weights$ een valide output geeft.\r\n\r\n\r\n\r\n\r\nDaarnaast is er nog een probleem die we nu hebben\r\nStel je voor je hebt een neuron die aan (1 moet geven als output) gaat als een bepaald aantal inputs aan is (dus dan maakt het niet uit welke inputs dat zijn).\r\nDus: \r\n$input = [1, 1, 1, 0]$ en $input = [1, 1, 1, 1]$ moet geven: $0$\r\nEn \r\n$input = [0, 0, 1, 1]$ en $input = [1, 1, 0, 0]$ moet geven: $1$\r\n\r\nJe ziet hier dat het niet uitmaakt welke volgorde de input staat dus de $weights$ bij elke input moeten niet uitmaken, dus de $weights$ zijn allemaal 1 of -1, dat weten we nu nog niet.\r\n$weights = [1, 1, 1, 1]$ of $weights = [-1, -1, -1, -1]$ \r\n\r\nAls je nu iets input dat niet $[a, b, c, d]$, waar $a + b + c + d > 0.5$ is is de output altijd 1, en dat willen we niet altijd.\r\nWat er is bedacht is een $bias$, dat is een getal die bij elke neuron aan het einde van de weight berekening bij het getal wordt opgeteld.\r\nDus bij ons voorbeeld:\r\n$input[0] * weight[0] + input[1] * weight[1] + input[2] * weight[2] + input[3] * weight[3] + $bias$ = output$\r\nMet $input = [1, 1, 1, 0]$, $bias = 3$ en $weight = -1$\r\nGeeft:\r\n$1 * -1 + 1 * -1 + 1 * -1 + 0 * -1 + 3 = 0$\r\nDe $input = [0, 0, 1, 1]$ (en dezelfde $weights$ en $bias$)\r\nGeeft:\r\n$0 * - 1 + 0 * -1 + 1 * -1 + 1 * -1 + 3= 1$\r\nEn dat zijn precies de outputs die we willen.\r\n\r\nDit was echt de basis basis van hoe ze werken. Met deze instellingen kan je elke neuron bij elke input elke output laten geven.\r\nEchte grote netwerken zoals die van google hebben soms wel miljoenen neurons met miljarder connecties (dus heel veel weight). Wat die weights zijn is een soort slim gokken (veel wiskunde), tot jij blij bent met de output. \r\n \r\n</div>\r\n</template>\r\n\r\n<script>\r\n\r\n\r\nexport default {\r\n    name: 'HoeAIPage.vue',\r\n    data(){\r\n        return {\r\n\r\n        }\r\n    }\r\n}\r\n</script>\r\n\r\n<style>\r\n\r\n</style>"]},"metadata":{},"sourceType":"module"}